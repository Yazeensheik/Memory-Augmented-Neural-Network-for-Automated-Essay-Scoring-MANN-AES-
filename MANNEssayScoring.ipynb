{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-Augmented Neural Network for Automated Essay Scoring\n",
    "\n",
    "This Jupyter Notebook provides a detailed walkthrough of training and evaluating a Memory-Augmented Neural Network (MANN) for automated essay scoring. The model uses GloVe embeddings and the quadratic kappa metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "from model import MANM\n",
    "from metric import kappa\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import torch\n",
    "\n",
    "# Argument parsing\n",
    "parser = argparse.ArgumentParser(description='MANN')\n",
    "parser.add_argument('--gpu_id', type=int, default=0)\n",
    "parser.add_argument('--set_id', type=int, default=1, help=\"essay set id, 1 <= id <= 8.\")\n",
    "parser.add_argument('--emb_size', type=int, default=300, help=\"Embedding size for sentences.\")\n",
    "parser.add_argument('--token_num', type=int, default=42, help=\"The number of token in glove (6, 42).\")\n",
    "parser.add_argument('--feature_size', type=int, default=100, help=\"Feature size.\")\n",
    "parser.add_argument('--epochs', type=int, default=200, help=\"Number of epochs to train for.\")\n",
    "parser.add_argument('--test_freq', type=int, default=20, help=\"Evaluate and print results every x epochs.\")\n",
    "parser.add_argument('--hops', type=int, default=3, help=\"Number of hops in the Memory Network.\")\n",
    "parser.add_argument('--lr', type=float, default=0.002, help=\"Learning rate.\")\n",
    "parser.add_argument('--batch_size', type=int, default=32, help=\"Batch size for training.\")\n",
    "parser.add_argument('--l2_lambda', type=float, default=0.3, help=\"Lambda for l2 loss.\")\n",
    "parser.add_argument('--num_samples', type=int, default=1, help=\"Number of samples selected as memories for each score.\")\n",
    "parser.add_argument('--epsilon', type=float, default=0.1, help=\"Epsilon value for Adam Optimizer.\")\n",
    "parser.add_argument('--max_grad_norm', type=float, default=10.0, help=\"Clip gradients to this norm.\")\n",
    "parser.add_argument('--keep_prob', type=float, default=0.9, help=\"Keep probability for dropout.\")\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU:{args.gpu_id}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.set_device(args.gpu_id)\n",
    "else:\n",
    "    print(\"!!! Using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Create output file for logs\n",
    "timestamp = time.strftime(\"%b_%d_%Y_%H_%M_%S\", time.localtime())\n",
    "out_file = \"./logs/set{}_{}.txt\".format(args.set_id, timestamp)\n",
    "with open(out_file, 'w', encoding='utf-8') as f:\n",
    "    for key, value in args.__dict__.items():\n",
    "        f.write(\"{}={}\".format(key, value))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# Load data\n",
    "train_essay_contents, train_essay_scores, train_essay_ids = data.load_train_data(args.set_id)\n",
    "dev_essay_contents, dev_essay_scores, dev_essay_ids = data.load_dev_data(args.set_id)\n",
    "test_essay_contents, test_essay_ids = data.load_test_data(args.set_id)\n",
    "min_score = min(train_essay_scores)\n",
    "max_score = max(train_essay_scores)\n",
    "if args.set_id == 7:\n",
    "    min_score, max_score = 0, 30\n",
    "elif args.set_id == 8:\n",
    "    min_score, max_score = 0, 60\n",
    "score_range = list(range(min_score, max_score + 1))\n",
    "# Get the vocabulary of training, dev, and test datasets.\n",
    "all_vocab = data.all_vocab(train_essay_contents, dev_essay_contents, test_essay_contents)\n",
    "print(f\"all_vocab len:{len(all_vocab)}\")\n",
    "\n",
    "# Get the length of the longest essay in the training set\n",
    "train_sent_size_list = list(map(len, [content for content in train_essay_contents]))\n",
    "max_sent_size = max(train_sent_size_list)\n",
    "mean_sent_size = int(np.mean(train_sent_size_list))\n",
    "print('max_score={} \\t min_score={}'.format(max_score, min_score))\n",
    "print('max train sentence size={} \\t mean train sentence size={}\\n'.format(max_sent_size, mean_sent_size))\n",
    "with open(out_file, 'a', encoding='utf-8') as f:\n",
    "    f.write('\\n')\n",
    "    f.write('max_score={} \\t min_score={}\\n'.format(max_score, min_score))\n",
    "    f.write('max sentence size={} \\t mean sentence size={}\\n'.format(max_sent_size, mean_sent_size))\n",
    "\n",
    "# Loading GloVe embeddings\n",
    "print(\"Loading Glove.....\")\n",
    "t1 = time.time()\n",
    "word_to_index, word_to_vec = data.load_glove(w_vocab=all_vocab, token_num=args.token_num, dim=args.emb_size)\n",
    "word_to_vec = np.array(word_to_vec, dtype=np.float32)\n",
    "t2 = time.time()\n",
    "print(f\"Finished loading Glove!, time cost = {(t2-t1):.4f}s\\n\")\n",
    "\n",
    "# Vectorize data\n",
    "train_contents_idx = data.vectorize_data(train_essay_contents, word_to_index, max_sent_size)\n",
    "dev_contents_idx = data.vectorize_data(dev_essay_contents, word_to_index, max_sent_size)\n",
    "test_contents_idx = data.vectorize_data(test_essay_contents, word_to_index, max_sent_size)\n",
    "\n",
    "# Prepare memory contents and scores\n",
    "memory_contents = []\n",
    "memory_scores = []\n",
    "for i in score_range:\n",
    "    for j in range(args.num_samples):\n",
    "        if i in train_essay_scores:\n",
    "            score_idx = train_essay_scores.index(i)\n",
    "            score = train_essay_scores.pop(score_idx)  # score=i\n",
    "            content = train_contents_idx.pop(score_idx)\n",
    "            memory_contents.append(content)\n",
    "            memory_scores.append(score)\n",
    "        else:\n",
    "            print(f\"score {i} is not in train data\")\n",
    "\n",
    "memory_size = len(memory_contents)  # actual score_range\n",
    "train_scores_index = list(map(lambda x: score_range.index(x), train_essay_scores))\n",
    "\n",
    "# Data size\n",
    "n_train = len(train_contents_idx)\n",
    "n_dev = len(dev_contents_idx)\n",
    "n_test = len(test_contents_idx)\n",
    "\n",
    "# Create batches\n",
    "start_list = list(range(0, n_train - args.batch_size, args.batch_size))\n",
    "end_list = list(range(args.batch_size, n_train, args.batch_size))\n",
    "batches = zip(start_list, end_list)\n",
    "batches = [(start, end) for start, end in batches]\n",
    "if end_list[len(end_list)-1] != n_train-1:\n",
    "    batches.append((end_list[len(end_list)-1], n_train-1))\n",
    "\n",
    "# Initialize model\n",
    "model = MANM(word_to_vec=word_to_vec, max_sent_size=max_sent_size, memory_num=memory_size, embedding_size=args.emb_size,\n",
    "             feature_size=args.feature_size, score_range=len(score_range), hops=args.hops,\n",
    "             l2_lambda=args.l2_lambda, keep_prob=args.keep_prob, device=device).to(device)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, eps=args.epsilon)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "# Begin training\n",
    "print(\"----------begin training----------\")\n",
    "t1 = time.time()\n",
    "dev_kappa_result = 0.0\n",
    "for ep in range(1, args.epochs+1):\n",
    "    t2 = time.time()\n",
    "    total_loss = 0\n",
    "    np.random.shuffle(batches)\n",
    "    for start, end in batches:\n",
    "        contents = np.array(train_contents_idx[start:end], dtype=np.int64)\n",
    "        scores_index = np.array(train_scores_index[start:end], dtype=np.int64)\n",
    "        batched_memory_contents = np.array([memory_contents]*(end-start), dtype=np.int64)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(contents, batched_memory_contents, scores_index)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    t3 = time.time()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=args.max_grad_norm)\n",
    "    scheduler.step(ep)\n",
    "    print(f\"epoch {ep}/{args.epochs}: all loss={total_loss:.3f}, \"\n",
    "          f\"loss/triple={(total_loss/train_essay_contents.__len__()):.6f}, \" f\"time cost={(t3-t2):.4f}\")\n",
    "    with open(out_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(\"epoch {}: total_loss={:.3f}, loss/triple={:.6f}\\n\".format(ep, total_loss, total_loss/train_essay_contents.__len__()))\n",
    "    # Begin evaluation\n",
    "    if ep % args.test_freq == 0 or ep == args.epochs:\n",
    "        print(\"------------------------------------\")\n",
    "        mid1 = round(n_dev/3)\n",
    "        mid2 = round(n_dev/3)*2\n",
    "        dev_batches = [(0, mid1), (mid1, mid2), (mid2, n_dev)]\n",
    "        all_pred_scores = []\n",
    "        for start, end in dev_batches:\n",
    "            dev_contents = np.array(dev_contents_idx[start:end], dtype=np.int64)\n",
    "            batched_memory_contents = np.array([memory_contents]*dev_contents.shape[0], dtype=np.int64)\n",
    "            pred_scores = model.test(dev_contents, batched_memory_contents).cpu().numpy()\n",
    "            pred_scores = np.add(pred_scores, min_score)\n",
    "            all_pred_scores += list(pred_scores)\n",
    "        dev_kappa_result = kappa(dev_essay_scores, all_pred_scores, weights='quadratic')\n",
    "        print(f\"kappa result={dev_kappa_result}\")\n",
    "        print(\"------------------------------------\")\n",
    "        with open(out_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(\"------------------------------------\\n\")\n",
    "            f.write(\"kappa result={}\\n\".format(dev_kappa_result))\n",
    "            f.write(\"------------------------------------\\n\")\n",
    "    if ep == args.epochs:\n",
    "        print(\"----------finish training----------\\n\")\n",
    "        print(\"\\n----------begin test----------\")\n",
    "        print(f\"results are written to file ./set\" + str(args.set_id) + \".tsv\")\n",
    "        file_out = open(\"./result/set\" + str(args.set_id) + \".tsv\", 'w', encoding='utf-8')\n",
    "        mid = round(n_test/2)\n",
    "        test_batches = [(0, mid), (mid, n_test)]\n",
    "        all_pred_scores = []\n",
    "        for start, end in test_batches:\n",
    "            test_contents = np.array(test_contents_idx[start:end], dtype=np.int64)\n",
    "            batched_memory_contents = np.array([memory_contents]*test_contents.shape[0], dtype=np.int64)\n",
    "            pred_scores = model.test(test_contents, batched_memory_contents).cpu().numpy()\n",
    "            pred_scores = np.add(pred_scores, min_score)\n",
    "            all_pred_scores += list(pred_scores)\n",
    "        for k in range(len(test_contents_idx)):\n",
    "            file_out.write(str(test_essay_ids[k]) + \"\\t\" + str(args.set_id) + \"\\t\" + str(all_pred_scores[k]) + \"\\n\")\n",
    "        file_out.close()\n",
    "        print(\"-----------finish test-----------\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
